{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9ce7f0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Image Captioning using ViT-GPT2\n",
    "\n",
    "### Introduction\n",
    "This project leverages a pre-trained Vision Transformer (ViT) and GPT-2 model to automatically generate captions for images. Using state-of-the-art natural language processing and computer vision technologies, this system can analyze images and generate meaningful descriptions. This project demonstrates the power of combining image processing and text generation models to create a sophisticated image captioning system.\n",
    "\n",
    "### Project Code Explanation\n",
    "\n",
    "#### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e14d3",
   "metadata": {},
   "source": [
    "\n",
    "We start by importing the necessary libraries. `torch` is used for tensor operations, `transformers` provides the pre-trained models, `PIL` handles image processing, and `os` is used for file system operations.\n",
    "\n",
    "#### 2. Loading the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a50d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1af63",
   "metadata": {},
   "source": [
    "Here, we load the pre-trained ViT-GPT2 model. If a GPU is available, it will be used to speed up computations.\n",
    "\n",
    "#### 3. Loading Image Processor and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09456d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75ace5",
   "metadata": {},
   "source": [
    "We load the image processor and tokenizer corresponding to the ViT-GPT2 model. The processor is used to preprocess images, and the tokenizer is used to decode generated captions.\n",
    "\n",
    "#### 4. Defining the Caption Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc92150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        pixel_values = pixel_values.to(device)\n",
    "\n",
    "        # Generate captions\n",
    "        outputs = model.generate(pixel_values, max_length=16, num_beams=4)\n",
    "        caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return f\"Error processing image: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09946bb",
   "metadata": {},
   "source": [
    "This function takes an image path as input, processes the image, and generates a caption using the model. If an error occurs, it returns an error message.\n",
    "\n",
    "#### 5. Generating Captions for Images in a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75980ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing images\n",
    "image_dir = \"persons\"  # Replace with your folder path\n",
    "output_dir = \"output_captions\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Iterate over images in the directory\n",
    "print(f\"Generating captions for images in '{image_dir}'...\")\n",
    "for image_file in os.listdir(image_dir):\n",
    "    if image_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        caption = generate_caption(image_path)\n",
    "        print(f\"Image: {image_file}\\nCaption: {caption}\\n\")\n",
    "\n",
    "        # Save the caption to a file\n",
    "        with open(os.path.join(output_dir, f\"{image_file}_caption.txt\"), \"w\") as f:\n",
    "            f.write(caption)\n",
    "\n",
    "print(f\"Captions saved in '{output_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fe2ed",
   "metadata": {},
   "source": [
    "This section iterates over all images in the specified directory, generates captions for each image, and saves the captions to text files in the output directory.\n",
    "\n",
    "### Conclusion\n",
    "This project effectively showcases the integration of advanced NLP and computer vision techniques to create an image captioning system. The application has numerous potential use cases, such as aiding visually impaired users, enhancing content accessibility, and automating image metadata generation. By including this project in your portfolio, you demonstrate proficiency in state-of-the-art AI technologies and your ability to apply them to solve real-world problems.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
